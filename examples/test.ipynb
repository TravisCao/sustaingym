{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\", {\"grid.color\": \".6\", \"grid.linestyle\": \":\"})\n",
    "\n",
    "from stable_baselines3 import PPO, A2C\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.utils import set_random_seed\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "\n",
    "from sustaingym.envs import ElectricityMarketEnv\n",
    "from sustaingym.envs.battery.plot_utils import plot_model_training_reward_curves, plot_reward_distribution, plot_state_of_charge_and_prices, plot_reward_over_episode, run_model_for_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveActionsExperienced(BaseCallback):\n",
    "    def __init__(self, log_dir: str, verbose: int = 1):\n",
    "        super(SaveActionsExperienced, self).__init__(verbose)\n",
    "        self.log_dir = log_dir\n",
    "        self.save_path = os.path.join(log_dir, 'actions')\n",
    "        self.constructed_log = False\n",
    "        self.time_steps = []\n",
    "        self.training_selling_prices = []\n",
    "        self.training_buying_prices = []\n",
    "        self.training_energy_lvl = []\n",
    "        self.training_dispatch = []\n",
    "    \n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "    \n",
    "    def _on_step(self) -> bool:\n",
    "        env = self.training_env\n",
    "        obs = env.get_attr('obs', 0)[0] # get current observation from the vectorized env\n",
    "        # count = env.get_attr('count', 0)[0]\n",
    "        prev_action = obs['previous action']\n",
    "        energy_lvl = obs['energy'][0]\n",
    "        dispatch = obs['previous agent dispatch'][0]\n",
    "\n",
    "        # print('step: ', count)\n",
    "        # print('selling price: ', prev_action[0])\n",
    "        # print('buying price: ', prev_action[1])\n",
    "\n",
    "        self.time_steps.append(self.num_timesteps)\n",
    "        self.training_selling_prices.append(prev_action[0])\n",
    "        self.training_buying_prices.append(prev_action[1])\n",
    "        self.training_energy_lvl.append(energy_lvl)\n",
    "        self.training_dispatch.append(dispatch)\n",
    "\n",
    "        if not self.constructed_log:\n",
    "            # first_row = np.array([count, prev_action[0], prev_action[1]])\n",
    "            # df = pd.DataFrame({'Step': first_row[0], 'Selling Price': first_row[1],\n",
    "            #                 'Buying Price': first_row[2]}, index=[0])\n",
    "            # df.to_csv(f'{self.save_path}/action_log.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "            np.savez(f'{self.save_path}/action_log',\n",
    "                step=self.time_steps,\n",
    "                selling_price=self.training_selling_prices,\n",
    "                buying_price=self.training_buying_prices,\n",
    "                energy_lvl=self.training_energy_lvl,\n",
    "                dispatch=self.training_dispatch)\n",
    "            \n",
    "            self.constructed_log = True\n",
    "\n",
    "        else:\n",
    "            # curr_row = np.array([count, prev_action[0], prev_action[1]])\n",
    "            # df = pd.read_csv(f'{self.save_path}/action_log.csv', sep='\\t', encoding='utf-8')\n",
    "            # df.loc[len(df)] = curr_row\n",
    "            # df.to_csv(f'{self.save_path}/action_log.csv', sep='\\t', encoding='utf-8', index=False)\n",
    "\n",
    "            np.savez(f'{self.save_path}/action_log',\n",
    "                step=self.time_steps,\n",
    "                selling_price=self.training_selling_prices,\n",
    "                buying_price=self.training_buying_prices,\n",
    "                energy_lvl=self.training_energy_lvl,\n",
    "                dispatch=self.training_dispatch)\n",
    "    \n",
    "        return True \n",
    "\n",
    "save_path = os.path.join(os.getcwd(), 'logs_PPO/')\n",
    "model_save_path = os.path.join(os.getcwd(), 'model_PPO_2019_5')\n",
    "\n",
    "env_2019 = BatteryStorageInGridEnv(month='2019-05', seed=195)\n",
    "env_2021 = BatteryStorageInGridEnv(month='2021-05', seed=215)\n",
    "\n",
    "# rescale action spaces to normalized [0,1] interval\n",
    "wrapped_env_2019 = gym.wrappers.RescaleAction(env_2019, min_action=0, max_action=1)\n",
    "wrapped_env_2021 = gym.wrappers.RescaleAction(env_2021, min_action=0, max_action=1)\n",
    "\n",
    "save_path_in_dist = os.path.join(save_path, 'in_dist/')\n",
    "save_path_out_dist = os.path.join(save_path, 'out_dist/')\n",
    "\n",
    "steps_per_ep = wrapped_env_2019.MAX_STEPS_PER_EPISODE\n",
    "\n",
    "log_actions_callback = SaveActionsExperienced(log_dir=save_path)\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "eval_callback_in_dist = EvalCallback(wrapped_env_2019, best_model_save_path=save_path_in_dist,\n",
    "log_path=save_path_in_dist, eval_freq=10*steps_per_ep, callback_after_eval=stop_train_callback)\n",
    "eval_callback_out_dist = EvalCallback(wrapped_env_2021, best_model_save_path=save_path_out_dist,\n",
    "log_path=save_path_out_dist, eval_freq=10*steps_per_ep)\n",
    "callback_list = CallbackList([log_actions_callback, eval_callback_in_dist, eval_callback_out_dist])\n",
    "\n",
    "model = PPO(\"MultiInputPolicy\", wrapped_env_2019, gamma=0.995, verbose=1)\n",
    "print(\"Training model\")\n",
    "model.learn(int(steps_per_ep), callback=callback_list)\n",
    "print(\"\\nTraining finished. \\n\")\n",
    "print(\"----- ----- ----- -----\")\n",
    "print(\"----- ----- ----- -----\")\n",
    "model.save(os.path.join(model_save_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = os.path.join(os.getcwd(), 'examples/discrete_logs_DQN/actions')\n",
    "\n",
    "results = np.load(f'{save_path}/action_log.npz')\n",
    "time_steps = results['step']\n",
    "sell_prices = results['selling_price']\n",
    "buy_prices = results['buying_price']\n",
    "energy_lvls = results['energy_lvl']\n",
    "disbatches= results['dispatch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time_steps = np.arange(1, len(steps) + 1)\n",
    "# fig, (ax, ax2, ax3) = plt.subplots(3)\n",
    "\n",
    "# ax.plot(time_steps[:100], sell_prices[:100], color='red')\n",
    "# ax2.plot(time_steps[:100], buy_prices[:100], color='green')\n",
    "# ax3.plot(time_steps[:100], energy_lvls[:100], color='blue')\n",
    "\n",
    "# # naming the x axis \n",
    "# ax2.set_xlabel('time step')\n",
    "# # naming the y axis \n",
    "# ax.set_ylabel('selling price ($)')\n",
    "# ax2.set_ylabel('buying price ($)')\n",
    "# ax3.set_ylabel('state of charge (MWh)')\n",
    "\n",
    "ax1 = plt.subplot(411)\n",
    "plt.plot(time_steps[288*100:288*101], sell_prices[288*100:288*101], color='red')\n",
    "plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "\n",
    "# share x only\n",
    "ax2 = plt.subplot(412, sharex=ax1)\n",
    "plt.plot(time_steps[288*100:288*101], buy_prices[288*100:288*101], color='green')\n",
    "# make these tick labels invisible\n",
    "plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "\n",
    "# share x only\n",
    "ax3 = plt.subplot(413, sharex=ax1)\n",
    "plt.plot(time_steps[288*100:288*101], disbatches[288*100:288*101], color='blue')\n",
    "# make these tick labels invisible\n",
    "plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "\n",
    "# share x and y\n",
    "ax4 = plt.subplot(414, sharex=ax1, sharey=ax1)\n",
    "plt.plot(time_steps[288*100:288*101], energy_lvls[288*100:288*101], color='orange')\n",
    "\n",
    "# naming the x axis \n",
    "ax4.set_xlabel('time step')\n",
    "# naming the y axis \n",
    "ax1.set_ylabel('selling price ($)')\n",
    "ax2.set_ylabel('buying price ($)')\n",
    "ax3.set_ylabel('dispatch (MWh)')\n",
    "ax4.set_ylabel('state of charge (MWh)')\n",
    "\n",
    "plt.subplots_adjust(bottom=0, right=0.8, top=1.5)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

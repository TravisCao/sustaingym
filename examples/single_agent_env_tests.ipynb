{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:272: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing environment with stable_baselines3 library\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from sustaingym.envs.battery.MO_single_agent_battery_storage_env import BatteryStorageInGridEnv\n",
    "\n",
    "env = BatteryStorageInGridEnv()\n",
    "\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Testing RL agent which randomly chooses actions\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = BatteryStorageInGridEnv(month='2019-05')\n",
    "\n",
    "episodes = 1\n",
    "\n",
    "# rewards_lst_1 = []\n",
    "# off_line_rewards_lst_1 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    ob = env.reset(seed=i)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    energy_rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    carbon_rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    terminal_rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "\n",
    "    while not done:\n",
    "        # random action as policy\n",
    "        # print(\"step: \", env.count)\n",
    "        action = env.action_space.sample()\n",
    "        # print(\"step: \", env.count)\n",
    "        # print(\"charging costs: \", env.bats_charge_costs)\n",
    "        # print(\"discharging costs: \", env.bats_discharge_costs)\n",
    "        # print(\"env load: \", env.load_demand)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards[env.count] = reward\n",
    "        energy_rewards[env.count] = info['energy reward']\n",
    "        carbon_rewards[env.count] = info['carbon reward']\n",
    "        terminal_rewards[env.count] = info['terminal reward']\n",
    "    \n",
    "    print(\"total reward: \", np.sum(rewards))\n",
    "    print(\"total energy reward: \", np.sum(energy_rewards))\n",
    "    print(\"total carbon reward: \", np.sum(carbon_rewards))\n",
    "    print(\"total terminal reward: \", terminal_rewards[-1])\n",
    "\n",
    "\n",
    "    # print(\"episode {} mean reward: {}\".format(i, np.mean(rewards)))\n",
    "    # rewards_lst_1.append(np.sum(rewards))\n",
    "    # ob = env.reset(seed=i)\n",
    "    # off_line_rewards_lst_1.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# # plot episode # versus total episode reward\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(list(range(episodes)), rewards_lst_1, width=0.5)\n",
    "# plt.title(\"Random Actions\")\n",
    "\n",
    "# # naming the y axis \n",
    "# plt.ylabel('total reward')\n",
    "\n",
    "# # plot episode # versus total offline episode reward\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(list(range(episodes)), off_line_rewards_lst_1, width=0.5)\n",
    "# plt.title(\"Offline Optimal\")\n",
    "\n",
    "# # naming the x axis \n",
    "# plt.xlabel('episode #')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, StopTrainingOnNoModelImprovement\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "# class PlottingCallback(BaseCallback):\n",
    "#     \"\"\"\n",
    "#     Callback for plotting the performance in realtime.\n",
    "\n",
    "#     :param verbose: (int)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, verbose=1):\n",
    "#         super(PlottingCallback, self).__init__(verbose)\n",
    "#         self._plot = None\n",
    "\n",
    "#     def _on_step(self) -> bool:\n",
    "#         # get the monitor's data\n",
    "#         x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "#         if self._plot is None: # make the plot\n",
    "#             plt.ion()\n",
    "#             fig = plt.figure(figsize=(6,3))\n",
    "#             ax = fig.add_subplot(111)\n",
    "#             line, = ax.plot(x, y)\n",
    "#             self._plot = (line, ax, fig)\n",
    "#             plt.show()\n",
    "#         else: # update and rescale the plot\n",
    "#             self._plot[0].set_data(x, y)\n",
    "#             self._plot[-2].relim()\n",
    "#             self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "#                                     self.locals[\"total_timesteps\"] * 1.02])\n",
    "#             self._plot[-2].autoscale_view(True,True,True)\n",
    "#             self._plot[-1].canvas.draw()\n",
    "\n",
    "# # Create log dir\n",
    "# log_dir = \"/tmp/gym/\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "# model.learn(int(1e10), callback=[eval_callback, PlottingCallback()])\n",
    "# model.save(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "# episodes = 10\n",
    "\n",
    "# rewards_lst_4 = []\n",
    "# off_line_rewards_lst_4 = []\n",
    "\n",
    "# for i in tqdm(range(episodes)):\n",
    "#     obs = env.reset(seed=i)\n",
    "#     done = False\n",
    "#     start = True\n",
    "#     rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "#     avg = np.zeros(1)\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         rewards[env.count - 1] = reward\n",
    "#     rewards_lst_4.append(np.sum(rewards))\n",
    "#     ob = env.reset(seed=i)\n",
    "#     off_line_rewards_lst_4.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# # plot episode # versus total episode reward\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(list(range(episodes)), rewards_lst_4, width=0.5)\n",
    "# plt.title(\"Random Actions\")\n",
    "\n",
    "# # naming the y axis \n",
    "# plt.ylabel('total reward')\n",
    "\n",
    "# # plot episode # versus total offline episode reward\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(list(range(episodes)), off_line_rewards_lst_4, width=0.5)\n",
    "# plt.title(\"Offline Optimal\")\n",
    "\n",
    "# # naming the x axis \n",
    "# plt.xlabel('episode #')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.ddpg.policies import MultiInputPolicy\n",
    "# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "# from stable_baselines3 import DDPG\n",
    "# import numpy as np\n",
    "\n",
    "# env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# # the noise objects for DDPG\n",
    "# n_actions = env.action_space.shape[-1]\n",
    "# action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# # model = DDPG(MultiInputPolicy, env, action_noise=action_noise, verbose=1)\n",
    "# model = DDPG(\"MultiInputPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "# model.learn(int(1e10), callback=eval_callback)\n",
    "# model.save(\"ddpg_single_agent_battery_env\")\n",
    "\n",
    "# # del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = DDPG.load(\"ddpg_single_agent_battery_env\")\n",
    "\n",
    "# episodes = 10\n",
    "\n",
    "# rewards_lst_2 = []\n",
    "# off_line_rewards_lst_2 = []\n",
    "\n",
    "# for i in tqdm(range(episodes)):\n",
    "#     obs = env.reset(seed=i)\n",
    "#     done = False\n",
    "#     start = True\n",
    "#     rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "#     avg = np.zeros(1)\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         rewards[env.count - 1] = reward\n",
    "#     rewards_lst_2.append(np.sum(rewards))\n",
    "#     ob = env.reset(seed=i)\n",
    "#     off_line_rewards_lst_2.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# # plot episode # versus total episode reward\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(list(range(episodes)), rewards_lst_2, width=0.5)\n",
    "# plt.title(\"Random Actions\")\n",
    "\n",
    "# # naming the y axis \n",
    "# plt.ylabel('total reward')\n",
    "\n",
    "# # plot episode # versus total offline episode reward\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(list(range(episodes)), off_line_rewards_lst_2, width=0.5)\n",
    "# plt.title(\"Offline Optimal\")\n",
    "\n",
    "# # naming the x axis \n",
    "# plt.xlabel('episode #')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import A2C\n",
    "# from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "# env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# model = A2C(\"MultiInputPolicy\", env, verbose=1)\n",
    "# model.learn(int(1e10), callback=eval_callback)\n",
    "# model.save(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = PPO.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "# episodes = 10\n",
    "\n",
    "# rewards_lst_3 = []\n",
    "# off_line_rewards_lst_3 = []\n",
    "\n",
    "# for i in tqdm(range(episodes)):\n",
    "#     obs = env.reset(seed=i)\n",
    "#     done = False\n",
    "#     start = True\n",
    "#     rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "#     avg = np.zeros(1)\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         rewards[env.count - 1] = reward\n",
    "#     rewards_lst_3.append(np.sum(rewards))\n",
    "#     ob = env.reset(seed=i)\n",
    "#     off_line_rewards_lst_3.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# # plot episode # versus total episode reward\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(list(range(episodes)), rewards_lst_3, width=0.5)\n",
    "# plt.title(\"Random Actions\")\n",
    "\n",
    "# # naming the y axis \n",
    "# plt.ylabel('total reward')\n",
    "\n",
    "# # plot episode # versus total offline episode reward\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(list(range(episodes)), off_line_rewards_lst_3, width=0.5)\n",
    "# plt.title(\"Offline Optimal\")\n",
    "\n",
    "# # naming the x axis \n",
    "# plt.xlabel('episode #')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get rewards for Testing on May 2021\n",
    "\n",
    "# model_ppo = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "# model_a2c = PPO.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "# episodes = 10\n",
    "\n",
    "# rewards_lst_ppo_21_5 = []\n",
    "# rewards_lst_a2c_21_5 = []\n",
    "# off_line_rewards_lst_21_5 = []\n",
    "\n",
    "# env1 = BatteryStorageInGridEnv(date='2021-05')\n",
    "# env2 = BatteryStorageInGridEnv(date='2021-05')\n",
    "\n",
    "# for i in tqdm(range(episodes)):\n",
    "#     obs = env1.reset(seed=i)\n",
    "#     done = False\n",
    "#     rewards = np.zeros(env1.MAX_STEPS_PER_EPISODE)\n",
    "#     while not done:\n",
    "#         action, _states = model_ppo.predict(obs)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs, reward, done, info = env1.step(action)\n",
    "#         rewards[env1.count - 1] = reward\n",
    "    \n",
    "#     obs2 = env2.reset(seed=i)\n",
    "#     done2 = False\n",
    "#     rewards2 = np.zeros(env2.MAX_STEPS_PER_EPISODE)\n",
    "#     while not done2:\n",
    "#         action, _states = model_a2c.predict(obs2)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs2, reward, done2, info = env2.step(action)\n",
    "#         rewards2[env2.count - 1] = reward\n",
    "#     rewards_lst_ppo_21_5.append(np.sum(rewards))\n",
    "#     rewards_lst_a2c_21_5.append(np.sum(rewards2))\n",
    "#     ob = env1.reset(seed=i)\n",
    "#     off_line_rewards_lst_21_5.append(env1._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# print(\"2021/5 PPO: \", rewards_lst_ppo_21_5)\n",
    "# print(\"2021/5 A2C: \", rewards_lst_a2c_21_5)\n",
    "# print(\"2021/5 Offline Optimal: \", off_line_rewards_lst_21_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, StopTrainingOnNoModelImprovement\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "# train PPO on May 2019 data\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05', seed=195)\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=5, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "# model.learn(int(1e10), callback=eval_callback)\n",
    "# model.save(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# generate evaluation for PPO trained on May 2019 and tested on May 2019 (in-dist)\n",
    "\n",
    "model_ppo = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_ppo_19_5 = []\n",
    "discharge_bids = []\n",
    "charge_bids = []\n",
    "state_of_charge = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    if i == 0:\n",
    "        state_of_charge.append(env.battery_charge[-1:][0])\n",
    "        discharge_bids.append(0.)\n",
    "        charge_bids.append(0.)\n",
    "    while not done:\n",
    "        action, _states = model_ppo.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        if i == 0:\n",
    "            discharge_bids.append(action[0])\n",
    "            charge_bids.append(action[1])\n",
    "            state_of_charge.append(env.battery_charge[-1:][0])\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_ppo_19_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "import matplotlib.ticker as plticker\n",
    "\n",
    "cmap = plt.get_cmap(\"tab10\")\n",
    "\n",
    "moers = []\n",
    "forecast_moers = []\n",
    "demand = []\n",
    "forecast_demand = []\n",
    "\n",
    "obs = env.reset(seed=0)\n",
    "\n",
    "for i in range(env.MAX_STEPS_PER_EPISODE):\n",
    "    moers.append(env._generate_moer_data(i))\n",
    "    forecast_moers.append(env._generate_moer_forecast_data(i+1))\n",
    "    demand.append(env._generate_load_data(i))\n",
    "    forecast_demand.append(env._generate_load_forecast_data(i+1))\n",
    "\n",
    "color_cycle = plt.rcParams['axes.prop_cycle']()\n",
    "\n",
    "fmt = mdates.DateFormatter('%H:%M:%S')\n",
    "\n",
    "df_load = env._get_demand_data()\n",
    "\n",
    "time_arr = df_load.columns[:-1]\n",
    "time_arr = [t + ':00.0' for t in time_arr]\n",
    "timeArray = [datetime.datetime.strptime(i, '%H:%M:%S.%f') for i in time_arr]\n",
    "\n",
    "fig, (ax1, ax2)  = plt.subplots(2)\n",
    "\n",
    "ax3 = ax1.twinx()\n",
    "ax4 = ax2.twinx()\n",
    "\n",
    "ax3.plot(timeArray, state_of_charge, label='State of Charge', color=cmap(0))\n",
    "ax1.plot(timeArray, discharge_bids, label='Discharge Bid Price', color=cmap(1))\n",
    "ax1.plot(timeArray, charge_bids, label='Charge Bid Price', color=cmap(2))\n",
    "\n",
    "ax2.plot(timeArray, demand, label='Demand', color=cmap(3))\n",
    "ax2.plot(timeArray, forecast_demand, label='Demand Forecast', color=cmap(3), linestyle='dashed')\n",
    "ax4.plot(timeArray, moers, label='MOER', color=cmap(4))\n",
    "ax4.plot(timeArray, forecast_moers, label='MOER Forecast', color=cmap(4), linestyle='dashed')\n",
    "\n",
    "ax1.set_ylabel('Energy Level (MWh)')\n",
    "ax2.set_ylabel('Demand (MWh)')\n",
    "ax3.set_ylabel('Bid Price ($/MWh)')\n",
    "ax4.set_ylabel('MOER')\n",
    "\n",
    "ax2.set_xlabel('Time')\n",
    "\n",
    "lines_labels = [ax.get_legend_handles_labels() for ax in fig.axes]\n",
    "lines, labels = [sum(lol, []) for lol in zip(*lines_labels)]\n",
    "\n",
    "ax1.xaxis.set_major_formatter(fmt)\n",
    "ax2.xaxis.set_major_formatter(fmt)\n",
    "loc = plticker.MultipleLocator(base=0.2) # this locator puts ticks at regular intervals\n",
    "ax1.xaxis.set_major_locator(loc)\n",
    "ax2.xaxis.set_major_locator(loc)\n",
    "\n",
    "fig.legend(lines, labels, bbox_to_anchor=(1.04,0.5), loc=\"center left\", borderaxespad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for A2C trained on May 2019 and tested on May 2019 (in-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05', seed=195)\n",
    "\n",
    "model_a2c = A2C.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_a2c_19_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_a2c.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_a2c_19_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate off-line optimal values for May 2019\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05', seed=195)\n",
    "\n",
    "offline_optimal_19_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    offline_optimal_19_5.append(env._calculate_off_optimal_total_episode_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for PPO trained on May 2019 and tested on May 2021 (out-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2021-05', seed=215)\n",
    "\n",
    "model_ppo_out = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_ppo_21_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_ppo_out.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_ppo_21_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for A2C trained on May 2019 and tested on May 2021 (out-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2021-05', seed=215)\n",
    "\n",
    "model_a2c_out = A2C.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_a2c_21_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_a2c_out.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_a2c_21_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate off-line optimal values for May 2019\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2021-05', seed=215)\n",
    "\n",
    "offline_optimal_21_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    offline_optimal_21_5.append(env._calculate_off_optimal_total_episode_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"2019/5 PPO: \", rewards_ppo_19_5)\n",
    "print(\"2019/5 A2C: \", rewards_a2c_19_5)\n",
    "print(\"2019/5 Offline Optimal: \", offline_optimal_19_5)\n",
    "print(\"2021/5 PPO: \", rewards_ppo_21_5)\n",
    "print(\"2021/5 A2C: \", rewards_a2c_21_5)\n",
    "print(\"2021/5 Offline Optimal: \", offline_optimal_21_5)\n",
    "\n",
    "models = ['Offline', 'PPO', 'A2C']\n",
    "in_dist = [np.mean(offline_optimal_19_5), np.mean(rewards_ppo_19_5), np.mean(rewards_a2c_19_5)]\n",
    "out_dist = [np.mean(offline_optimal_21_5), np.mean(rewards_ppo_21_5), np.mean(rewards_a2c_21_5)]\n",
    "\n",
    "x_axis = np.arange(len(models))\n",
    "\n",
    "plt.bar(x_axis - 0.2, in_dist, width=0.4, label='In-Dist', color='blue')\n",
    "plt.bar(x_axis + 0.2, out_dist, width=0.4, label='Out-Dist', color='orange')\n",
    "\n",
    "in_dist_err = [np.std(offline_optimal_19_5), np.std(rewards_ppo_19_5), np.std(rewards_a2c_19_5)]\n",
    "out_dist_err = [np.std(offline_optimal_21_5), np.std(rewards_ppo_21_5), np.std(rewards_a2c_21_5)]\n",
    "\n",
    "plt.errorbar(x_axis - 0.2, in_dist, yerr=in_dist_err, fmt='o', color='gray')\n",
    "plt.errorbar(x_axis + 0.2, out_dist, yerr=out_dist_err, fmt='o', color='gray')\n",
    "\n",
    "plt.xticks(x_axis, models)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "rewards_ppo_19_5 = np.array(rewards_ppo_19_5)\n",
    "rewards_ppo_21_5 = np.array(rewards_ppo_21_5)\n",
    "offline_optimal_19_5 = np.array(offline_optimal_19_5)\n",
    "offline_optimal_21_5 = np.array(offline_optimal_21_5)\n",
    "\n",
    "data1 = np.array([offline_optimal_19_5, offline_optimal_21_5])\n",
    "data2 = np.array([rewards_ppo_19_5, rewards_ppo_21_5, rewards_ppo_19_5, rewards_ppo_21_5])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "xticklabels1 = ['Offline 2019', 'Offline 2021']\n",
    "xticklabels2 = ['PPO in-dist', 'PPO out-dist', 'A2C in-dist', 'A2C out-dist']\n",
    "\n",
    "df = pd.DataFrame(data = data1.T, columns = xticklabels1)\n",
    "df2 = pd.DataFrame(data = data2.T, columns = xticklabels2)\n",
    "\n",
    "ax1.set_xticks([1, 2])\n",
    "ax1.set_xticklabels(xticklabels1)\n",
    "\n",
    "ax1.set_ylabel('Reward ($)')\n",
    "\n",
    "ax1.violinplot(df, showmedians=True)\n",
    "\n",
    "ax2.set_xticks([1, 2, 3, 4])\n",
    "ax2.set_xticklabels(xticklabels2)\n",
    "\n",
    "ax2.set_ylabel('Reward ($)')\n",
    "\n",
    "ax2.violinplot(df2, showmedians=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sustaingym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c910351b0c3a4aade2cf03b555240fe9951314ae7b50b4f56bc279231ceafe8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

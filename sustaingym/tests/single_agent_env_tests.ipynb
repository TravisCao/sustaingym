{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/env_checker.py:272: UserWarning: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) cf https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing environment with stable_baselines3 library\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from envs.MO_single_agent_battery_storage_env import BatteryStorageInGridEnv\n",
    "\n",
    "env = BatteryStorageInGridEnv()\n",
    "\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:14<00:00,  1.46s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZoAAAEWCAYAAABfdFHAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAxeUlEQVR4nO3de7xUdb3/8ddbUERFRbwhiJjiBdQQCe0cO+lB0KyDEV4wT9LVNDuntOMtKm+h5i/T0spMOZKWpJ4MSsjQNLuoCIYXMBMVFSQFQcEbCn5+f3y/sxm2s/ceZu+19wDv5+MxD2a+a62Z7wxr5rPXd33W96OIwMzMrCgbdXQHzMxs/eZAY2ZmhXKgMTOzQjnQmJlZoRxozMysUA40ZmZWKAeaDYyk8yTd2NH9WFuSXpP0vo7uhxVLyf9KWippem47RdKLeR/oISkk7Z6XXS3pmx3b6+oV2d/yz6XeONDUAUnzJL2Zv0j/lHS9pC06ul9tQdKukt6V9KO12OYeSZ8vb4uILSLi6bbvobUnSZ+W9KikN/K+/mNJW5etcjAwDOgdEUMkbQx8Dxie94GXy58vIk6OiAsL6mtvST+X9LKk1yVNl/Sxtdj+05L+XN5WZH/rmQNN/fiPiNgCGAjsD5zTsd1pMycCS4HRkrp0dGes40j6GvAd4AxgK+AgYBdgmqRN8mq7APMi4vX8eAdgU2B2O/d1G+DPwNvAAGBb4HLgF5KObs++rBciwrcOvgHzgMPKHl8K3F72+GzgKWA5MAcYWbbs06QvxHdJP+jPAB8pW74r8Me87TTgKuDGsuUjSF/iV4B7gL0b9esM4BHgdeA60hd/an6+O4HuLby3p4BTgBeBoxstOwqYBSzL6x0BjANWAW8BrwFX5XUD2D3f3wr4GbAIeBb4BrBRlZ/Hp4Gnc/+fAU7o6P//DeEGbJn/P49t1L4F8BLwWeBz+f99VV73przfRX78hwr7wvXAt/P9Q4D5wNfycy4EPlP2Wl3yfvFc3h+vBro20d8LgcdK+1VZ+1l5n1NZX/4771OLgf9H+gN+70bv5ZVm+ntmWX8/DhwJ/ANYAny97LWHAPeRvqsLSd/lTcqWN3wu9Xbr8A74tmagAXoDjwLfL1t+DLBT3oGPy1++nnnZp4F3gC8AnUg/6i+UfRHuIw09dAH+Lf/A3piX7ZGfaxiwcd7h55Z23tyv+0nBpVf+MjxEOuLqAvwBOLeZ9/UhYAXQHbgSmFy2bAjwan7tjfLz75WX3QN8vtFzlf+4/AyYBHQD+uYv5eda+jyAzUlBbc+8bk9gQEf//28IN9IfESuBzhWWTQBuKvv/+3PZsr75/75zWVtzgWYlcEHen48E3iD/MQRcAUwGtsn7zm+Ai5vo7/3A+RXad82vv2dZX+7Oz9kn74ufr/Remunvt3J/v0D64+kXuX8DSMHqfXn9A0hHgZ3z5/I48NVKn0u93Tx0Vj9+LWk58DzpB/3c0oKIuCUiXoiIdyPil8CTpB/qkmcj4qcRsYr0pe0J7CCpD/AB4JsRsSIi7iV9uUqOIx05TYuId0h/7XUF/qVsnSsj4sWIWAD8CXggIv4WESuA20hBpyljgKkRsZT05fmIpO3zss8B4/NrvxsRCyLi7y19SJI65X6fExHLI2IecBnwqZY+j7zsXWAfSV0jYmFEtOuQzAZsW2BxRKyssGxhXt4W3gEuiIh3ImIK6WhiT0ki/ZCfFhFLImI5cBEwupn+Lmyir6XlJd/Jz/kcKZgdv5b9HZe/fxPz834/79uzSaMN+wFExMyIuD8iVub9/ifAh9fitTqMA039+HhEdCP9lbMXZTuypBMlzZL0iqRXgH1Yc0f/Z+lORLyR725BOgpaGqvHuyEd9pfsVP44It4lBbpeZeu8WHb/zQqPKyYtSOpKOhL7eX7u+0hDFp/Mq+xMGi5bW9sCmzR6H8826nPFzyN/DscBJwMLJd0uaa8a+mBrbzGwraTOFZb1zMvbwsuNgtkbpH10O2AzYGbZ9+h3ub2p/vZsoq+l5SXPl91/lvS9Wpv+rsr338z/VvyOSdpD0m9zEsUyUqBsqwBdKAeaOhMRfyQdXn8XQNIuwE+BLwM9ImJr0tixqni6hUB3SZuXtfUpu/8C6eQr+bVECgALan8HDUaSxuV/lL8Y/yQFgxPz8ueB3ZrYtrkpxReT/grcpaytD1X2OSLuiIhhpB+Mv5M+WyvefaRh1E+UN+Z98yPAXQW//mLSj/aAiNg637aKlIBTyZ3AKEmNfyOPJe27/yhr27nsfh/S9wqa349r8WPSPtsvIrYEvk51vwMdzoGmPl0BDJM0kHReIUhjt0j6DOmIpkUR8SwwAzhf0iaSDgb+o2yVm4GPShqa00i/Rvox+GsbvIcxwHhgX1Im3UDgX4GBkvYlJRZ8Jr/2RpJ6lR1dvAhUvGYm//V3MzBOUrcciE8HWrw2SNIOkkbkH7cVpGGVVS1sZm0gIl4FzgeulHSEpI0l9QVuIZ0Qv6Hg13+X9EfF5aXh27zPHd7EJpeT/lC6TtKOkjaVdDwwFjgj8kmR7AxJ3SXtDHwF+GVufxHoXZZR11rdSOcYX8vflVPa6HkL50BThyJiEemE9zcjYg7pHMR9pB13X+Ava/F0nwQOJGWwnJuft/Q6TwD/STpRv5gUhP4jIt5uTf8l9QKGAldExD/LbjNJwxVjImI68BnSF/pVUmZc6Sjl+8DR+aK9H1R4if8iJTE8Tcow+wUpqLVkI1IwfYH0eXwY+FKNb9PWUkRcSvor/LukH8wHSEcHQ/M5v6KdRUp2uT8PPd0J7NlEX18mXdOzKSnT82XSHzSfyudJy00CZpIyKG8n/REFKVlmNvBPSW0xNPg/pO/zclLQbNyPuqU1A7OZmVVLUpCGsuZ2dF/qmY9ozMysUBtEoMljwk9Imivp7I7uj5nZhmS9HzrL1138g3Rh4HzgQeD4fO7DzMwKtiEc0QwB5kbE0/kk90TS1CdmZtYOKl08tb7pxZoXVM0nZWE1kHQScBLA5ptvfsBee1W+hu/RBa82+0L79tqqyWXNbVvrdkW9Zmu2rbfXbM22Rb3mzJkzF0dEUxcKFmbbbbeNvn37tvfL2gaiuf16Qwg0lS5oWmO8MCKuAa4BGDx4cMyYMaPiE/U9+/ZmX2jGJR9tcllz29a6XVGv2Zpt6+01W7NtUa8p6dkmFxaob9++NLVvm7VWc/v1hjB0Np81r9ztzeord83MrGAbQqB5EOiXC3BtQppEb3IH98nWAyuXLeKfN53Dgp+ezAvXfollMyYBsOrN5QwbNox+/foxbNgwli5d2rDNxRdfDGlSzyfKr0qXdEAuCDZX0g/ydEBI6iLpl7n9gXw1fWmbMZKezLcx7fS2zdbaeh9o8gR7XwbuIE2rfbNn7LU2sVEnuh/6OXp94Wp2/NR3Wf7Q7by9+DmW3X8LQ4cO5cknn2To0KFccsklAMyZM4eJEydCulr8CNI8cJ3ys/2YdJ6wX74dkds/R5oYdXfSLArfgYbCXOeSzjcOAc6V1L093rbZ2lrvAw1AREyJiD0iYreIGNfR/bH1Q+cttqHLjqlE+0ZdNmPjHjuzavnLvDH3AcaMSQcYY8aM4de//jUAkyZNYvTo0QAREc+QpkMZIqknsGVE3Jfn0PoZqQAWpAzJCfn+rcDQfLRzODAtT0+/lFTUrhSczOrKBhFozIq28tUXefvFp+my056sev0VevZMs8n37NmTl156CYAFCxaw887lpwuZT8qK7JXvN26HsqzJfHT+KtCDytmU5aUSgJRRKWmGpBmLFi1q9fs0q4UDjVkrvfv2myy67SK2GfoFNuqyWZPrNXFxdNB8ZmRTy1rMpsyveU1EDI6Iwdtt1+4Z1WaAA41Zq8SqlSy67SI2738Im+2ZCpN22nxrFi5MhRgXLlzI9tunoqK9e/fm+efLD0IaMiDn5/uN26EsazIXDduKNPO0syltneFAY1ajiODlqd9n4x47s+WQkQ3tm+1+IBMmpNMqEyZM4Kij0kQUI0aMKCUDSNKupJP+0yNiIbBc0kH5/MuJpKnnIWVIljLKjgb+kM/j3AEMz3VQugPDc5tZ3dkQLtg0K8SKBXN4ffbdbLxdX1743/8CoPu/nciWBx3NtGnXct1119GnTx9uueUWAAYMGMCxxx7LI488MoBUl+fUsjK+p5Aqq3YFpuYbpNomN0iaSzqSKWUTLJF0ISl9H+CCiFhS9Hs2q4UDjVmNNu09gF3O+m3FZXfdVbky8dixY/nGN77xWEQMLm+PiBlUqJwaEW8Bx1R6rogYT3UF38yq1tysF/NamN2jKR46MzOzQjnQmJlZoRxozMysUA40ZmZWKAcaMzMrlAONmZkVyoHGzMwK5UBjZmaF8gWbZrbOKeKiQiuOj2jMzKxQPqIxs1bZUI4uNpT3WQQf0ZiZWaHqLtBI+n+S/i7pEUm3Sdq6bNk5kuZKekLS4WXtB0h6NC/7QZ5qHUldJP0ytz8gqW/7vyMzsw1b3QUaUu3zfSJiP+AfwDkAkvqTpkgfQKqN/iNJnfI2PwZOItX36Mfq2umfA5ZGxO7A5cB32utNmJlZUneBJiJ+n2ujA9zP6sqDRwETI2JFRDwDzAWGSOoJbBkR9+WCUD8DPl62zYR8/1ZgaOlox8zM2ke9JwN8Fvhlvt+LFHhK5ue2d/L9xu2lbZ4HiIiVkl4FegCLy19E0kmkIyL69OnTtu/AzKydNZe4AO2fvNAhgUbSncCOFRaNjYhJeZ2xwErg56XNKqwfzbQ3t82aDRHXANcADB48+D3Lzcysdh0SaCLisOaWSxoDfAwYmofDIB2p7Fy2Wm/ghdzeu0J7+TbzJXUGtiKVwzUzs3ZSd+doJB0BnAWMiIg3yhZNBkbnTLJdSSf9p0fEQmC5pIPy+ZcTgUll24zJ948G/lAWuMxaZfGUK3j+yhN44bovNbQtmvQdXvjf/2LgwIH07duXgQMHAjBv3jy6du1aetxf0tWlbWrJmpQ0RtKT+TYGszpWj+dorgK6ANPy9+3+iDg5ImZLuhmYQxpSOzUiVuVtTgGuB7oCU/MN4DrgBklzSUcyo9vtXdh6b4t9D6PboI/x8u3fa2jb7qizAJh1yUf52te+xlZbbdWwbLfddmPWrFlImhMRJ5c9VSlr8n5gCilrciplWZOSRpOyJo+TtA1wLjCYNBQ8U9LkiFha4Ns1q1ndBZqcitzUsnHAuArtM4B9KrS/BRzTph00yzbdeR9WvvpixWURwc0338wf/vCHZp+jPGsyPy5lTU4lZU2el1e9FbgqH+0cDkyLiCV5m2mk4HRTK9+SWSHqbujMbH3wpz/9iR122IF+/fo1tD3zzDPsv//+AHtK+lBu7kWVWZNAKWuyob3CNmuQdJKkGZJmLFq0qNXvy6wWdXdEY7Y+uOmmmzj++OMbHvfs2ZPnnnuOHj16IOl54BeSBlBb1mRV2ZTgjMp6UG+pxh3BgcasjcW7q/jVr37FzJkzG9q6dOlCly5dSg/fAF4C9qC2rMn5wCGNtrmnrd+HWVvx0JlZG3tr3iz22msvevdeHT8WLVrEqlWl3BU2IWVNPl1j1uQdwHBJ3SV1B4bnNrO65EBjVqNFky/lnzf8D+8sWcD8H45h+cO/B+D1x+9dY9gM4N5772W//fbj/e9/P8BuwMmlk/mkrMlrSdMqPcWaWZM9ctbk6cDZAHm7C4EH8+2CsucyqzseOjOr0XYjzqzYvu1HT+Pkk9ccdx81ahSjRo0CQNLjEfGb0rJasiYjYjwwvta+m7UnH9GYmVmhHGjMzKxQDjRmZlYoBxozMyuUA42ZmRXKgcbMzArlQGNmZoXydTRmZnWquXnS1qU50nxEY2ZmhXKgMTOzQjnQmJlZoeo20Ej6H0khaduytnNy/fQnJB1e1r7WNdfNzKx91GWgkbQzMAx4rqytPzAaGEAqW/sjSZ3y4lLN9X75dkRub6i5DlxOqrluZmbtqC4DDSkonMmaVQOPAiZGxIqIeIY0pfqQ8prruVZHqeZ6aZsJ+f6twNDS0Y6ZmbWPugs0kkYACyLi4UaLmqqTXkvN9cav6brqZmYF6ZDraCTdCexYYdFY4OukioHv2axCW0v106uqre666mZmxemQQBMRh1Vql7QvsCvwcB7h6g08JGkIq+unl5Rqq9dSc93MzNpJXQ2dRcSjEbF9RPSNiL6kQDEoIv5Jqp8+OmeS7Uo66T+9xprrZq22eMoVPH/lCbxw3Zca2l7588+Z/8MTGThwIAMHDmTKlCkNyy6++GJ23313gH1amzUpaYykJ/NtDGZ1bJ2ZgiYiZku6GZgDrAROjYhVefEpwPVAV1K99fKa6zfkmutLSFlrZm1ii30Po9ugj/Hy7d9bo73b4I8z6+7r1mibM2cOEydOZPbs2Wy66ab/IGVN7pH34VLW5P3AFFLW5FTKsiYljSZlTR4naRvgXGAwaSh4pqTJEbG00DdsVqO6DjT5qKb88ThgXIX11rrmullrbbrzPqx89cWq1p00aRKjR4+mS5cuAG8DC0lZk/PIWZMAkkpZk1NJWZPn5ae4FbgqH+0cDkyLiCV5m2mk4HRTm7wxszZWV0NnZuuD5Q/9lv3224/PfvazLF2aDjIWLFjAzjuXn2JsVdZkUxmY7+GMSqsHDjRmbajb/kfS64s/ZdasWfTs2ZOvfe1rADRxarDWrMmqsinz614TEYMjYvB2223XYv/NiuBAY9aGOm3eHW3UiY022ogvfOELTJ8+HYDevXvz/PPlByFrlTVJo6zJpjIwzeqSA41ZG1r52urs+dtuu4199kmnDkeMGMHEiRNZsWIFwCa0LmvyDmC4pO6SupOuO7uj8DdnVqO6TgYwq2eLJl/KiuceZdWby5j/wzFsdfAJrHj+Ud5+8Wn2m3IOffv25Sc/+QkAAwYM4Nhjj6V///4AewAja82ajIglki4EHszrXVBKDDCrRw40ZjXabsSZ72nr9v40qcUjFaofjh07lrFjxyLpsYgoBZOasiYjYjwwvta+m7UnD52ZmVmhHGjMzKxQDjRmZlYoBxozMyuUA42ZmRXKgcbMzArlQGNmZoVyoDEzs0I50JiZWaEcaMzMrFCegsbMNhh9z7692eXzKkwdZK1Xl0c0kv5L0hOSZku6tKz9nFw//YnW1lw3M7P2UXeBRtKhpBK2+0XEAOC7ub0/afbaAaSytT+S1ClvVqq53i/fjsjtDTXXgctJNdfNzKwd1V2gIU2ZfklErACIiJdy+1HAxIhYERHPAHNJNdd7kmuu51odpZrrpW0m5Pu3AkNLRztmZtY+6jHQ7AF8KA91/VHSB3J7U3XSa6m5vgbXVTczK06HJANIuhPYscKisaQ+dQcOAj4A3CzpfdRWP72q2uoRcQ1wDcDgwYMr1l43s7bV3Il5n5RfvzQZaCRdSYUf5ZKI+O9aXzQiDmvmdU8BfpWHwaZLehfYlqbrpFdTc31+o5rrZmbWTpobOpsBzAQ2BQYBT+bbQGBV05u12q+BfweQtAepvvpiUv300TmTbFdaV3PdzMzaSZOBJiImRMQE0g/6oRFxZURcCQwlBZuijAfeJ+kxYCIwJpLZwM3AHOB3wKmNaq5fS0oQeIo1a673yDXXTwfOLrDftoFZPOUKnr/yBF647ksNbUvvHs+Cn57Mfvvtx8iRI3nllVcAmDdvHl27dmXgwIEA/SVdXdqmlvR8SWMkPZlvYzCrY9Wco9kJ6MbqIactclshIuJt4D+bWDYOGFehfa1rrpu11hb7Hka3QR/j5du/19C2ad+BbP3hMTxy6QjOOussLr74Yr7znZRVv9tuuzFr1iwkzYmIk8ueqpSefz8whZSeP5Wy9HxJo0np+cdJ2gY4FxhMGt6eKWlyRCyt9b34fIkVqZqss0uAv0m6XtL1wEPARYX2ymwdsOnO+9Cpa7c12rruOghtlC7vOuigg5g/f36lTRvUmJ5/ODAtIpbk4DKN1deOmdWdZgONpI2AJ4ADgdvy7YN5SM3MmjF+/Hg+8pGPNDx+5pln2H///QH2lPSh3FxLen5Tqf7v4dR9qwfNDp1FxLuSLouID7L6BLuZtWDcuHF07tyZE044AYCePXvy3HPP0aNHDyQ9D/xC0gBqS8+vKm0fnLpv9aGaczS/lzSK1SnHZtaM1x69i98uuY+77rqL0kQUXbp0oUuXLqVV3gBeIl2cXEt6/nzgkEbb3FPEezFrC9WcozkduAVYIWmZpOWSlhXcL7N10ptPz2TZA7cyefJkNttss4b2RYsWsWpVw1UBm5CyOZ+uMT3/DmC4pO6SugPDc5tZXWrxiCYiurW0jtmGaNHkS1nx3KOsenMZ8384hq0OPoFl999CrHqHYcOGASkh4Oqrr+bee+/lW9/6Fp07dwbYDRgdEaVMzlOA64GupGyz8vT8G3J6/hLSpLJExBJJFwIP5vUuKHsus7pT1RQ0+a+mfqSLNwGIiHuL6pTZumC7EWe+p63b+4cDMKtRSvCoUaMYNWoUAJIej4jflJbVkp4fEeNJ15yZ1b0WA42kzwNfIY0DzyLNQXYf+ep9MzOz5lRzjuYrpMktn42IQ4H9AedJmplZVaoJNG/lQ3gkdYmIvwN7FtstMzNbX1Rzjma+pK1Jk11Ok7SU1emXZmZmzaom62xkvnuepLtJufy/K7RXZma23qgmGeAC4E/AXyPij8V3yczM1ifVnKOZBxwPzJA0XdJlko4qtltmZra+aDHQRMT4iPgscChwIymv/8aiO2ZmZuuHaobOrgX6Ay+ShtCOJpUKMDMza1E1Q2c9gE7AK6RpMBbnKcvNzMxaVM3Q2ciIOBC4FNgauFtS89WcWkHSQEn3S5qV62gMKVt2Ti5r+4Skw8va17oUrpmZtY9qhs4+BnwI+DegO/AH0hBaUS4Fzo+IqZKOzI8PkdSfNKngAFIp6Tsl7RERq1jLUrgF9t3MzBqp5oLNjwD3At+PiPa4UDOALfP9rVh9cehRwMSIWAE8k2e0HSJpHrkULoCkUincqXmb8/L2twJXSZLr6piZtZ9qLtg8VdIupISAFyR1BTpHxPKC+vRV4A5J3yUN7f1Lbu9FOmIpKZWvfYcqS+FKKpXCXVxQ383MrJFqhs6+QBqW2oZUR6M3cDUwtNYXlXQnsGOFRWPz854WEf8n6VhSTY7DqK2sbVUlbyWdRHqP9OnTp8X+m5lZ9aoZOjsVGAI8ABART0ravjUvGhGHNbUsD319JT+8Bbg23y+VtS0plbytpRRu4/64rrqZWUGqSW9eERFvlx7kH+wif4xfAD6c7/878GS+PxkYnTPJdiUVYpteYylcMzNrJ9UEmj9K+jrQVdIw0lHGb1rYpjW+AFwm6WHgIvKQVkTMBm4G5pAm9Tw1Z5xBKoV7LTAXeIo1S+H2yIkDpwNnF9hv28AsnnIFz195Ai9c96WGtlVvLufFid+gX79+DBs2jKVLlzYsu/jii9l9990B9mlter6kMZKezLcxmNWxagLNWaRCZ48CXySlD3+jqA5FxJ8j4oCIeH9EHBgRM8uWjYuI3SJiz4iYWtY+IyL2ycu+XDpqiYi3IuKYiNg9IoZExNNF9ds2PFvsexjbH3P+Gm3L7r+FTfu+nyeffJKhQ4dyySWXADBnzhwmTpzI7NmzAf4B/EhSp7xZKT2/X74dkdsb0vOBy0np+UjaBjgXOJA0rH1uLrduVpeaDTSSNgIejYif5h/so/N9Dz/ZBm/TnfehU9dua7S9MfcBNt8n5cmMGTOGX//61wBMmjSJ0aNH06VLF4C3SUffQyT1JKfn5+9VKT0fUnr+hHz/VmBoPto5HJgWEUsiYikwjdXByazuNBtoIuJd4GFJTsUyq8Kq11+h8xbbANCzZ09eeuklABYsWMDOO5fnsjSk4feiyvR8oJSe39BeYZs1SDopz7AxY9EiV2C3jlFN1llPYLak6cDrpcaIGFFYr8zWM00MAtSanl9V2n5+XWdUWoerJtCc3/IqZgbQafOtWflayqBfuHAh22+frgTo3bs3zz9ffhDSqvT8+cAhjba5p23fiVnbqWZSzT9WurVH58zWNZvtfiCvP3YXABMmTOCoo1KNwBEjRjBx4kRWrFgBsAmtS8+/AxguqXtOAhie28zqUjVHNGZWwaLJl7LiuUdZ9eYy5v9wDFsdfAJbHnQ0iyddQr9+/ejTpw+33HILAAMGDODYY4+lf//+AHsAIxul518PdCWl5pen59+Q0/OXkCaVJSKWSLoQeDCvd0FEvOdCZLN64UBjVqPtRpxZsX2H0Rfx5CUffU/72LFjGTt2LJIea5yeD+zTeP2IeItU0fY9ImI8ML7Grpu1q2quozEzM6tZk0c0kh6lciaLgIiI/QrrlZmZrTeaGzr7WLv1wszM1ltNBpqIeLY9O2JmZuunFs/R5LTLByW9JultSaskLWuPzpmZ2bqvmmSAq4DjSdP1dwU+D1xZZKfMzGz9UVV6c0TMldQp5/3/r6S/FtwvMzNbT1QTaN6QtAkwS9KlwEJg82K7ZWZm64tqhs4+ldf7MmlSzZ2BTxTZKTMzW39UE2g+nguILYuI8yPidJz6bGZmVaom0FQqE/vpNu6HmZmtp5oMNJKOl/QbYFdJk8tu9wAvt+ZFJR0jabakdyUNbrTsnFwj/QnXVTczW/c1lwzwV9KJ/22By8ralwOPtPJ1HyOd5/lJeaOk/qQZagcAOwF3StojZ7uV6qrfD0whla6dSllddUmjSXXVjyurqz6YNJXOTEmTc+lbMzNrJ00e0UTEsxFxT0R8EPg70C3f5ueysjWLiMcj4okKi44CJkbEioh4BtdVNzNb51UzM8AxwHTSdOXHAg9IOrqg/jRVC9111c3M1lHVXEfzDeADEfESgKTtgDtJRw9NknQnsGOFRWMjYlKFdqitRrrrqpuZ1bFqAs1GpSCTvUx1JaAPq6E/pRrpJa6rbma2jqsmvfl3ku6Q9GlJnwZuZ3Wp2bY2GRidM8l2xXXVbR30zsvzGThwYMNtyy235IorruC8886jV69eAP0lzZJ0ZGmbtsy2NKs31RyZnEHKDtsPeD9wTURUrmFbJUkjJc0HPgjcLumO/FqzgZuBOcDvgFMb1VW/lpQg8BRr1lXvkeuqnw6cnZ9rCVCqq/4grqtu7WTjHr2ZNWsWs2bNYubMmWy22WaMHDkSgNNOOw1gTkQMjIgp8J5syyOAH0nqlJ+ulG3ZL99KCS0N2ZbA5aRsS7O61OLQmaTvRMRZwK8qtNUkIm4Dbmti2ThgXIV211W3dc5dd93Fbrvtxi677NLcag3ZlsAz+Y+mIZLmkbMtASSVsi2n5m3Oy9vfClwlSflo3qyuVDN0NqxC20fauiNm66OJEydy/PHHNzy+6qqrIA2djc9DutC22ZZmdae5mQFOkfQosKekR8puz9D6CzbN1ntvv/02kydP5phj0gH3KaecwlNPPQVpaHghqy+EbstsyzU4dd/qQXNDZ78gHaJfTD7vkS33uQ6zlk2dOpVBgwaxww47ADT8m/0U+G2+35bZlmtw6r7Vg+ZmBng1IuZFxPF5loDSzUHGrAo33XTTGsNmCxcuLF88kjQVE7RttqVZ3amqwqaZrZ033niDadOm8ZOfrJ7O78wzz2TWrFkA/YFDgS9CyraUVMq2XMl7sy2vJ5VRn8qa2ZY35MSBJaSsNbO65EBjVoDNNtuMl19ec5LzG264AQBJcyJiRPmytsy2NKs31WSdmZmZ1cyBxszMCuVAY2ZmhXKgMTOzQjnQmJlZoRxozMysUA40ZmZWKAcaMzMrlAONmZkVyoHGzMwK5UBjZmaF6pBAI+kYSbMlvStpcFn7MEkzc430mZL+vWzZWtdOlzRG0pP5NgYzM2t3HXVE8xjwCeDeRu2Lgf+IiH1JU6DfULZsrWqnS9oGOBc4EBgCnFtW0dDMzNpJhwSaiHg8Ip6o0P63iCgVdpoNbJqPWHqSa6fnmhul2umQaqdPyPdvBYbmo53DgWkRsSQilgLTWB2czMysndTzOZpRwN8iYgW11U5vqg77e7jcrZlZcQqrRyPpTmDHCovGRsSkCu3l2w4gDYENLzVVWK2l2ulV1VQHl7s1MytSYYEmIg6rZTtJvYHbgBMj4qncXEvt9PnAIY22uaeWPpmZWe3qauhM0tbA7cA5EfGXUnuNtdPvAIZL6p6TAIbnNjMza0cdld48UtJ84IPA7ZJKAeDLwO7ANyXNyrft87JTgGuBucBTrFk7vUeunX46cDZARCwBLgQezLcLcptZ4fr27cu+++7LwIEDGTw4ZfAvWbKEYcOGAewjaVp5FqSkc3KK/hOSDi9rX+u0frN6U9jQWXMi4jbS8Fjj9m8D325im7WunR4R44HxreqsWY3uvvtutt1224bHl1xyCUOHDuXOO+98DLiL9EfRWZL6A6OBAcBOwJ2S9oiIVaxO678fmELKnJxKWVq/pNGkc5rHtd+7M6teXQ2dma3PJk2axJgxDdcNT2DNFP2JEbEiIp4hHbUPqTGt36zuONCYFUASw4cP54ADDuCaa64B4MUXX6Rnz55Aw3nH0rBwU6n4taT1N+6HU/etw3XI0JnZ+u4vf/kLO+20Ey+99BLDhg1jr732am71WlL0q0rfd+q+1QMf0ZgVYKeddgJg++23Z+TIkUyfPp0ddtiBhQsXApCHxV7Kq5dS9EtK6fvVpPXTKK3frO440Ji1sXfffovly5cD8Prrr/P73/+effbZhxEjRjBhQum0CmNYM0V/dM4k25U0l9/0GtP6zeqOh87M2tiqN17h4IMPBmDlypV88pOf5IgjjuADH/gAxx57LKTsyVfJ2ZIRMVvSzcAcYCVwas44g5TWfz3QlZRtVp7Wf0NO619Cylozq0sONGZtbOOtd+Thhx9+T3uPHj246667kPRYRAwtXxYR44BxjbepJa3frN546MzMzArlQGNmZoVyoDEzs0I50JiZWaEcaMzMrFAONGZmVigHGjMzK5QDjZmZFcqBxszMCuVAY2ZmheqQKWgkHQOcB+wNDMnTbJQv70Oa9+m8iPhubjuA1XM+TQG+EhEhqQupINQBwMvAcRExL28zBvhGftpvR8QE1jHzLvloR3fBzKxVOuqI5jHgE8C9TSy/nNWTB5aUStr2y7cjcntDSdu83XcAJG0DnAscCAwBzi2v0W5mZu2jQwJNRDweEU9UWibp48DTwOyytlpK2h4OTIuIJRGxFJjG6uBkZmbtpK7O0UjaHDgLOL/RolpK2jZVHtfMzNpRYedoJN0J7Fhh0diImFShHVKAuTwiXksHJaufrsK6LZW0rarUbe7rSaRhOfr06dNE13y+xMysFoUFmog4rIbNDgSOlnQpsDXwrqS3gP+j5ZK28xuVtJ0PHNJom3ua6Kvrqq/D/AeAWX2rq8JnEfGh0n1J5wGvRcRV+fFySQcBD5BK2l6ZVy2VtL2PspK2ku4ALipLABgOnNMub2QD5h99M2usQ87RSBopaT7wQeD2HBRacgpwLTAXeIo1S9r2yCVtTwfOBoiIJcCFwIP5dkFuMyvUymWLOPTQQ9l7770ZMGAA3//+9wE477zz6NWrF0B/SbMkHVnaRtI5kuZKekLS4WXtB0h6NC/7QU50QVIXSb/M7Q9I6tu+79Kseh1yRBMRtwG3tbDOeY0er3VJ24gYD4yvuaNmtdioE5dddhmDBg1i+fLlHHDAAQwbNgyA0047jTPOOGNORAwurS6pPzAaGADsBNwpaY+IWMXqtP77SdePHUH6I6shrV/SaFJa/3Ht+C7NqlZXWWdm64POW2zDoEGDAOjWrRt77703CxYsaG6To4CJEbEiIp4hHbUPqTGt36zuONCYFWjevHn87W9/48ADDwTgqquugjR0Nr7s/GFTqfi1pPWvQdJJkmZImrFo0aI2e19ma6OukgHWZz5JvuF57bXXGDVqFFdccQVbbrklp5xyCt/85jfp3LnzHGAhcBnwWWpL0a8qfd8ZlVYPHGhsg1bUHwDvvPMOo0aN4oQTTuATn/gEADvssEP5Kj8Ffpvvl1L0S0rp+/NZ+7R+s7rjoTOzNhYRfO5zn2Pvvffm9NNPb2hfuHBh+WojSXP+QUrRH50zyXYlzeU3PSIWAsslHZTPv5wITCrbZky+35DWX9ibMmsFH9GYtbEVC+Zww89vYN9992XgwIEAXHTRRdx0003MmjULoD9wKPBFgIiYLelm0ozlK4FTc8YZpLT+60mzlk9lzbT+G3Ja/xJS1ppZXXKgMWtjm/YeQKWDiyOPTJfNSJoTESPKl0XEOGBc421qSes3qzcONOsxJyCYWT1woLGKHKTMrK04GcDMzArlQGNmZoVyoDEzs0I50JiZWaGcDGB1wwkIZusnH9GYmVmhHGjMzKxQDjRmZlYon6Mxq4HPJ5lVr0OOaCQdI2m2pHclDW60bD9J9+Xlj0raNLevde10SWMkPZlvYzAzs3bXUUNnjwGfAO4tb8x1NW4ETo6IAcAhwDt5cal2er98OyK3N9ROBy4n1U5H0jbAucCBwBDg3LKKhmZm1k46JNBExOMR8USFRcOBRyLi4bzeyxGxqsba6YcD0yJiSUQsBaaxOjiZmVk7qbdkgD2AkHSHpIcknZnba6md3lQd9vdwXXUzs+IUlgwg6U5gxwqLxkbEpArtpf4cDHwAeAO4S9JMYFmFdVuqnV5VTXVwXXUzsyIVFmgi4rAaNpsP/DEiFgNImgIMIp23Wdva6fNJ53jKt7mnhj6ZmVkr1NvQ2R3AfpI2y0Hjw8CcGmun3wEMl9Q9JwEMz21mZtaOOiq9eaSk+cAHgdsl3QGQT9p/D3gQmAU8FBG3581OAa4F5gJPsWbt9B65dvrpwNn5uZYAF+bnehC4ILeZrRckHSHpiZzaf3ZH98esKR1ywWZE3Abc1sSyG0lDZY3b17p2ekSMB8a3qrNmdUhSJ+CHwDDSMPGDkiZHxJyO7ZnZe9Xb0JmZVWcIMDcino6It4GJpFR/s7qjdDrDSiQtAp6tcvVtgcUFdmdt1Vt/oP76VA/92SUitmvNE0g6GjgiIj6fH38KODAivtxovZNIFzoD7AlUun6tknr4nMq5Py3r6D41uV97rrNG1uYHQNKMiBjc8prto976A/XXp3rrTytUlb5fnrq/Vk9eZ5+T+9OyeuxTiYfOzNZNpbT+kvKUf7O64kBjtm56EOgnaVdJmwCjSan+ZnXHQ2ets9ZDEgWrt/5A/fWp3vpTk4hYKenLpGvDOgHjI2J2G75EvX1O7k/L6rFPgJMBzMysYB46MzOzQjnQmJlZoRxoqtDSVB9KfpCXPyJpUIF92VnS3ZIez1VIv1JhnUMkvSppVr59q6j+lL3mvFwBdZakGRWWt+dntGfZe58laZmkrzZap90/o3pTT/t1fr2627frab/Or7du7tsR4VszN9KJ1qeA9wGbAA8D/RutcyRp7jUBBwEPFNifnsCgfL8b8I8K/TkE+G07f07zgG2bWd5un1GF/79/ki4m69DPqJ5u9bZf59eru327Xvfrsv/DdWLf9hFNy6qZ6uMo4GeR3A9srVQVtM1FxMKIeCjfXw48ThMF3epMu31GjQwFnoqIamd72FDU1X4N6+y+3VH7NaxD+7YDTcuqqdRZdTXPtiSpL7A/8ECFxR+U9LCkqZIGFN0X0lXpv5c0M0970liHfEak60tuamJZe39G9aRu92uoq327XvdrWIf2bV9H07JqpvqouppnW5G0BfB/wFcjonEF0odIh9OvSToS+DXQr8j+AP8aES9I2h6YJunvEXFveZcrbFP0Z7QJMAI4p8LijviM6kld7tdQd/t23e3XsO7t2z6iaVk1U32063QgkjYmfRF/HhG/arw8IpZFxGv5/hRgY0nbFtWf/Dov5H9fIpWAGNJolY6YMuUjpJpGLzZe0BGfUZ2pu/0a6m/frtP9GtaxfduBpmXVTPUxGTgxZ6AcBLwaqSpom5MkUrG3xyPie02ss2NeD0lDSP/PLxfRn/wam0vqVrpPqmb6WKPV2u0zKnM8TQwttPdnVIfqar+G+tu363i/hnVs3/bQWQuiiak+JJ2cl18NTCFln8wF3gA+U2CX/hX4FPCopFm57etAn7L+HA2cImkl8CYwOnI6SkF2AG7L+3Zn4BcR8bsO/IyQtBmpKNgXy9rK+9Pen1FdqcP9Gupv3667/RrWzX3bU9CYmVmhPHRmZmaFcqAxM7NCOdCYmVmhHGjMzKxQDjRmZlYoBxoz2yBIukDSYW3wPK/VuN0XJX1a0kBJV7e2H+sSpzebma0FSa9FxBY1bHcjcC7wMWBxRPy8zTtXp3xEY2brJEn/KWl6rrnyE0mdcvtrki6T9JCkuyRtl9uvl3R0vn+JpDlKNWS+m9t2yes/kv/tk9t3lXSfpAclXdioD2fk9kcknd9EP0/LF6COJE2vcz4wdkM6qnGgMbN1jqS9geNIk14OBFYBJ+TFm5PmARsE/JF0FFG+7TakH/0BEbEf8O286CrSlP/7AT8HfpDbvw/8OCI+QKr/Unqe4aTJKocAA4EDJP1b475GxOWkK/nvyn19MiL6R8TJrfkM1iUONGa2LhoKHAA8mI8WhpKKuAG8C/wy378ROLjRtsuAt4BrJX2CNHUMwAeBX+T7N5Rt96+snlfshrLnGZ5vfyPNmLwXTc+SPAh4OM+dtrSqd7ge8VxnZrYuEjAhIipNk9/YGiei8zxvQ0jBaTTwZeDfW9iu0slsARdHxE+a7GQqL/B7YHtScDse6JaD46iIeKqK/q/zfERjZuuiu4Cj8w85kraRtEtethFpYkmATwJ/Lt9Qqd7NVnkK/a+Shr0A/koKPJCG4Urb/aVRe8kdwGfz8yGpV6k/JRHxUh4ue4g0xHYj8JmIGLihBBnwEY2ZrYMiYo6kb5CqX24EvAOcCjwLvA4MkDQTeJV0LqdcN2CSpE1JRyWn5fb/BsZLOgNYxOqZmL8C/ELSV0gn80t9+H0+V3RfnuH5NeA/gZfKXywnKfSIiMWS/gWoWAJhfeb0ZjNbr9SafmzF8dCZmZkVykc0ZmZWKB/RmJlZoRxozMysUA40ZmZWKAcaMzMrlAONmZkV6v8DsRvYX16lZJwAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Testing RL agent which randomly chooses actions\n",
    "\"\"\"\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_lst_1 = []\n",
    "off_line_rewards_lst_1 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    ob = env.reset(seed=i)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "\n",
    "    while not done:\n",
    "        # random action as policy\n",
    "        action = env.action_space.sample()\n",
    "        # print(\"step: \", env.count)\n",
    "        # print(\"charging costs: \", env.bats_charge_costs)\n",
    "        # print(\"discharging costs: \", env.bats_discharge_costs)\n",
    "        # print(\"env load: \", env.load_demand)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    # print(\"episode {} mean reward: {}\".format(i, np.mean(rewards)))\n",
    "    rewards_lst_1.append(np.sum(rewards))\n",
    "    ob = env.reset(seed=i)\n",
    "    off_line_rewards_lst_1.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# plot episode # versus total episode reward\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(list(range(episodes)), rewards_lst_1, width=0.5)\n",
    "plt.title(\"Random Actions\")\n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('total reward')\n",
    "\n",
    "# plot episode # versus total offline episode reward\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(list(range(episodes)), off_line_rewards_lst_1, width=0.5)\n",
    "plt.title(\"Offline Optimal\")\n",
    "\n",
    "# naming the x axis \n",
    "plt.xlabel('episode #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.70s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from stable_baselines3 import PPO\n",
    "# from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, StopTrainingOnNoModelImprovement\n",
    "# from stable_baselines3.common.monitor import Monitor\n",
    "# from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
    "\n",
    "# class PlottingCallback(BaseCallback):\n",
    "#     \"\"\"\n",
    "#     Callback for plotting the performance in realtime.\n",
    "\n",
    "#     :param verbose: (int)\n",
    "#     \"\"\"\n",
    "#     def __init__(self, verbose=1):\n",
    "#         super(PlottingCallback, self).__init__(verbose)\n",
    "#         self._plot = None\n",
    "\n",
    "#     def _on_step(self) -> bool:\n",
    "#         # get the monitor's data\n",
    "#         x, y = ts2xy(load_results(log_dir), 'timesteps')\n",
    "#         if self._plot is None: # make the plot\n",
    "#             plt.ion()\n",
    "#             fig = plt.figure(figsize=(6,3))\n",
    "#             ax = fig.add_subplot(111)\n",
    "#             line, = ax.plot(x, y)\n",
    "#             self._plot = (line, ax, fig)\n",
    "#             plt.show()\n",
    "#         else: # update and rescale the plot\n",
    "#             self._plot[0].set_data(x, y)\n",
    "#             self._plot[-2].relim()\n",
    "#             self._plot[-2].set_xlim([self.locals[\"total_timesteps\"] * -0.02, \n",
    "#                                     self.locals[\"total_timesteps\"] * 1.02])\n",
    "#             self._plot[-2].autoscale_view(True,True,True)\n",
    "#             self._plot[-1].canvas.draw()\n",
    "\n",
    "# # Create log dir\n",
    "# log_dir = \"/tmp/gym/\"\n",
    "# os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# model = PPO(\"MultiInputPolicy\", env, verbose=1)\n",
    "# model.learn(int(1e10), callback=[eval_callback, PlottingCallback()])\n",
    "# model.save(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "# del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "# episodes = 10\n",
    "\n",
    "# rewards_lst_4 = []\n",
    "# off_line_rewards_lst_4 = []\n",
    "\n",
    "# for i in tqdm(range(episodes)):\n",
    "#     obs = env.reset(seed=i)\n",
    "#     done = False\n",
    "#     start = True\n",
    "#     rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "#     avg = np.zeros(1)\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         rewards[env.count - 1] = reward\n",
    "#     rewards_lst_4.append(np.sum(rewards))\n",
    "#     ob = env.reset(seed=i)\n",
    "#     off_line_rewards_lst_4.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# # plot episode # versus total episode reward\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(list(range(episodes)), rewards_lst_4, width=0.5)\n",
    "# plt.title(\"Random Actions\")\n",
    "\n",
    "# # naming the y axis \n",
    "# plt.ylabel('total reward')\n",
    "\n",
    "# # plot episode # versus total offline episode reward\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(list(range(episodes)), off_line_rewards_lst_4, width=0.5)\n",
    "# plt.title(\"Offline Optimal\")\n",
    "\n",
    "# # naming the x axis \n",
    "# plt.xlabel('episode #')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000004vscode-remote?line=14'>15</a>\u001b[0m \u001b[39m# model = DDPG(MultiInputPolicy, env, action_noise=action_noise, verbose=1)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000004vscode-remote?line=15'>16</a>\u001b[0m model \u001b[39m=\u001b[39m DDPG(\u001b[39m\"\u001b[39m\u001b[39mMultiInputPolicy\u001b[39m\u001b[39m\"\u001b[39m, env, action_noise\u001b[39m=\u001b[39maction_noise, verbose\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000004vscode-remote?line=16'>17</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(\u001b[39mint\u001b[39;49m(\u001b[39m1e10\u001b[39;49m), callback\u001b[39m=\u001b[39;49meval_callback)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000004vscode-remote?line=17'>18</a>\u001b[0m model\u001b[39m.\u001b[39msave(\u001b[39m\"\u001b[39m\u001b[39mddpg_single_agent_battery_env\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000004vscode-remote?line=19'>20</a>\u001b[0m \u001b[39m# del model # remove to demonstrate saving and loading\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/ddpg/ddpg.py:130\u001b[0m, in \u001b[0;36mDDPG.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    118\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    119\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    127\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    128\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 130\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    131\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    132\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    133\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    134\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    135\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    136\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    137\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    138\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    139\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    140\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/td3/td3.py:211\u001b[0m, in \u001b[0;36mTD3.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    200\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    208\u001b[0m     reset_num_timesteps: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    209\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OffPolicyAlgorithm:\n\u001b[0;32m--> 211\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    212\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    213\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    214\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    215\u001b[0m         eval_env\u001b[39m=\u001b[39;49meval_env,\n\u001b[1;32m    216\u001b[0m         eval_freq\u001b[39m=\u001b[39;49meval_freq,\n\u001b[1;32m    217\u001b[0m         n_eval_episodes\u001b[39m=\u001b[39;49mn_eval_episodes,\n\u001b[1;32m    218\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    219\u001b[0m         eval_log_path\u001b[39m=\u001b[39;49meval_log_path,\n\u001b[1;32m    220\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    221\u001b[0m     )\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:346\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    343\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    345\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 346\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    347\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    348\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    349\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    350\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    351\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    352\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    353\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    354\u001b[0m     )\n\u001b[1;32m    356\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    357\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/off_policy_algorithm.py:579\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    576\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    578\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    582\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:162\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \n\u001b[1;32m    158\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 162\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:43\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     42\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 43\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     44\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     45\u001b[0m         )\n\u001b[1;32m     46\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     47\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     48\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:90\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     89\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     92\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/sustaingym/sustaingym/tests/../envs/MO_single_agent_battery_storage_env.py:462\u001b[0m, in \u001b[0;36mBatteryStorageInGridEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minit\n\u001b[1;32m    461\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_type \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 462\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mcontains(action)\n\u001b[1;32m    464\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcount \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    466\u001b[0m \u001b[39m# ensure selling cost (a) for charge is at least as large as buying cost (b)\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from stable_baselines3.ddpg.policies import MultiInputPolicy\n",
    "# from stable_baselines3.common.noise import NormalActionNoise, OrnsteinUhlenbeckActionNoise\n",
    "# from stable_baselines3 import DDPG\n",
    "# import numpy as np\n",
    "\n",
    "# env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "# stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "# eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "# # the noise objects for DDPG\n",
    "# n_actions = env.action_space.shape[-1]\n",
    "# action_noise = NormalActionNoise(mean=np.zeros(n_actions), sigma=0.1 * np.ones(n_actions))\n",
    "\n",
    "# # model = DDPG(MultiInputPolicy, env, action_noise=action_noise, verbose=1)\n",
    "# model = DDPG(\"MultiInputPolicy\", env, action_noise=action_noise, verbose=1)\n",
    "# model.learn(int(1e10), callback=eval_callback)\n",
    "# model.save(\"ddpg_single_agent_battery_env\")\n",
    "\n",
    "# # del model # remove to demonstrate saving and loading\n",
    "\n",
    "# model = DDPG.load(\"ddpg_single_agent_battery_env\")\n",
    "\n",
    "# episodes = 10\n",
    "\n",
    "# rewards_lst_2 = []\n",
    "# off_line_rewards_lst_2 = []\n",
    "\n",
    "# for i in tqdm(range(episodes)):\n",
    "#     obs = env.reset(seed=i)\n",
    "#     done = False\n",
    "#     start = True\n",
    "#     rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "#     avg = np.zeros(1)\n",
    "#     while not done:\n",
    "#         action, _states = model.predict(obs)\n",
    "#         if action.shape[0] == 1:\n",
    "#             action = action.reshape((2,))\n",
    "#         obs, reward, done, info = env.step(action)\n",
    "#         rewards[env.count - 1] = reward\n",
    "#     rewards_lst_2.append(np.sum(rewards))\n",
    "#     ob = env.reset(seed=i)\n",
    "#     off_line_rewards_lst_2.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# # plot episode # versus total episode reward\n",
    "# plt.subplot(1, 2, 1)\n",
    "# plt.bar(list(range(episodes)), rewards_lst_2, width=0.5)\n",
    "# plt.title(\"Random Actions\")\n",
    "\n",
    "# # naming the y axis \n",
    "# plt.ylabel('total reward')\n",
    "\n",
    "# # plot episode # versus total offline episode reward\n",
    "# plt.subplot(1, 2, 2)\n",
    "# plt.bar(list(range(episodes)), off_line_rewards_lst_2, width=0.5)\n",
    "# plt.title(\"Offline Optimal\")\n",
    "\n",
    "# # naming the x axis \n",
    "# plt.xlabel('episode #')\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 287      |\n",
      "|    ep_rew_mean        | 2.49e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 209      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | -24.7    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0143  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.07e-05 |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/evaluation.py:65: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=1000, episode_reward=2409.92 +/- 74.85\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.41e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -1.13    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.0393   |\n",
      "|    std                | 0.997    |\n",
      "|    value_loss         | 0.000208 |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 287      |\n",
      "|    ep_rew_mean     | 2.5e+03  |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 200      |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 322      |\n",
      "|    ep_rew_mean        | 3.13e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 123      |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -5.55    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.014    |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 6.81e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=2575.21 +/- 170.36\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.58e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | 0.105    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | -0.00405 |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 1.11e-05 |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 310      |\n",
      "|    ep_rew_mean     | 2.88e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 400      |\n",
      "|    time_elapsed    | 19       |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 327      |\n",
      "|    ep_rew_mean        | 3.15e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 114      |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -0.133   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0408   |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 0.0003   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=2389.82 +/- 72.68\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.39e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 3000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -9.92    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | 0.00152  |\n",
      "|    std                | 0.99     |\n",
      "|    value_loss         | 2.03e-05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 318      |\n",
      "|    ep_rew_mean     | 2.97e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 600      |\n",
      "|    time_elapsed    | 29       |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 329      |\n",
      "|    ep_rew_mean        | 3.16e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 110      |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 31       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.82    |\n",
      "|    explained_variance | -10.2    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | -0.00836 |\n",
      "|    std                | 0.993    |\n",
      "|    value_loss         | 4.37e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=2420.96 +/- 121.50\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.42e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 4000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.81    |\n",
      "|    explained_variance | -2.85    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.0161  |\n",
      "|    std                | 0.988    |\n",
      "|    value_loss         | 5.22e-05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 322      |\n",
      "|    ep_rew_mean     | 3.06e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 800      |\n",
      "|    time_elapsed    | 38       |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 330      |\n",
      "|    ep_rew_mean        | 3.18e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 108      |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 4500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -4.68    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.0241   |\n",
      "|    std                | 0.995    |\n",
      "|    value_loss         | 0.000149 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=2647.75 +/- 131.91\n",
      "Episode length: 287.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| eval/                 |           |\n",
      "|    mean_ep_length     | 287       |\n",
      "|    mean_reward        | 2.65e+03  |\n",
      "| time/                 |           |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.83     |\n",
      "|    explained_variance | -1.65e+04 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | -0.00385  |\n",
      "|    std                | 0.996     |\n",
      "|    value_loss         | 2.84e-06  |\n",
      "-------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 324      |\n",
      "|    ep_rew_mean     | 3.11e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 48       |\n",
      "|    total_timesteps | 5000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 330      |\n",
      "|    ep_rew_mean        | 3.25e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 107      |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 51       |\n",
      "|    total_timesteps    | 5500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | -60.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0194  |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 7.69e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=2486.03 +/- 110.32\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.49e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 6000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | -59.8    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.00691 |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 0.00012  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 326      |\n",
      "|    ep_rew_mean     | 3.17e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1200     |\n",
      "|    time_elapsed    | 58       |\n",
      "|    total_timesteps | 6000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 3.27e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 60       |\n",
      "|    total_timesteps    | 6500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.86    |\n",
      "|    explained_variance | -0.646   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0219  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.92e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=2426.43 +/- 60.56\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.43e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 7000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -0.0437  |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0192   |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 3.9e-05  |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 327      |\n",
      "|    ep_rew_mean     | 3.19e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1400     |\n",
      "|    time_elapsed    | 68       |\n",
      "|    total_timesteps | 7000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 3.28e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 106      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0.0276   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.00426 |\n",
      "|    std                | 0.999    |\n",
      "|    value_loss         | 3.72e-06 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=2424.62 +/- 146.32\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.42e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 8000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.84    |\n",
      "|    explained_variance | 0.363    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.013   |\n",
      "|    std                | 1        |\n",
      "|    value_loss         | 3.62e-05 |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 328      |\n",
      "|    ep_rew_mean     | 3.22e+03 |\n",
      "| time/              |          |\n",
      "|    fps             | 102      |\n",
      "|    iterations      | 1600     |\n",
      "|    time_elapsed    | 77       |\n",
      "|    total_timesteps | 8000     |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 331      |\n",
      "|    ep_rew_mean        | 3.29e+03 |\n",
      "| time/                 |          |\n",
      "|    fps                | 105      |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 8500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.85    |\n",
      "|    explained_variance | -3.84    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.0104   |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 1.28e-05 |\n",
      "------------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=2547.32 +/- 138.80\n",
      "Episode length: 287.00 +/- 0.00\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 287      |\n",
      "|    mean_reward        | 2.55e+03 |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 9000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.83    |\n",
      "|    explained_variance | -0.275   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.0194   |\n",
      "|    std                | 0.998    |\n",
      "|    value_loss         | 8.57e-05 |\n",
      "------------------------------------\n",
      "Stopping training because there was no new best model in the last 4 evaluations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:17<00:00,  1.78s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnNoModelImprovement\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05')\n",
    "\n",
    "stop_train_callback = StopTrainingOnNoModelImprovement(max_no_improvement_evals=3, min_evals=5, verbose=1)\n",
    "eval_callback = EvalCallback(env, eval_freq=1000, callback_after_eval=stop_train_callback, verbose=1)\n",
    "\n",
    "model = A2C(\"MultiInputPolicy\", env, verbose=1)\n",
    "model.learn(int(1e10), callback=eval_callback)\n",
    "model.save(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_lst_3 = []\n",
    "off_line_rewards_lst_3 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i)\n",
    "    done = False\n",
    "    start = True\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    avg = np.zeros(1)\n",
    "    while not done:\n",
    "        action, _states = model.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_lst_3.append(np.sum(rewards))\n",
    "    ob = env.reset(seed=i)\n",
    "    off_line_rewards_lst_3.append(env._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "# plot episode # versus total episode reward\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(list(range(episodes)), rewards_lst_3, width=0.5)\n",
    "plt.title(\"Random Actions\")\n",
    "\n",
    "# naming the y axis \n",
    "plt.ylabel('total reward')\n",
    "\n",
    "# plot episode # versus total offline episode reward\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(list(range(episodes)), off_line_rewards_lst_3, width=0.5)\n",
    "plt.title(\"Offline Optimal\")\n",
    "\n",
    "# naming the x axis \n",
    "plt.xlabel('episode #')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 3/10 [00:08<00:19,  2.72s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 2)) of distribution Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan]], device='cuda:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000006vscode-remote?line=17'>18</a>\u001b[0m rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(env1\u001b[39m.\u001b[39mMAX_STEPS_PER_EPISODE)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000006vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000006vscode-remote?line=19'>20</a>\u001b[0m     action, _states \u001b[39m=\u001b[39m model_ppo\u001b[39m.\u001b[39;49mpredict(obs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000006vscode-remote?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m action\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bec2-3-142-99-116.us-east-2.compute.amazonaws.com/home/ubuntu/sustaingym/sustaingym/tests/single_agent_env_tests.ipynb#ch0000006vscode-remote?line=21'>22</a>\u001b[0m         action \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39mreshape((\u001b[39m2\u001b[39m,))\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:579\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\n\u001b[1;32m    560\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    561\u001b[0m     observation: np\u001b[39m.\u001b[39mndarray,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    564\u001b[0m     deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    565\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[np\u001b[39m.\u001b[39mndarray, Optional[Tuple[np\u001b[39m.\u001b[39mndarray, \u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m]]]:\n\u001b[1;32m    566\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[39m        (used in recurrent policies)\u001b[39;00m\n\u001b[1;32m    578\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 579\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mpredict(observation, state, episode_start, deterministic)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/policies.py:338\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    335\u001b[0m observation, vectorized_env \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobs_to_tensor(observation)\n\u001b[1;32m    337\u001b[0m \u001b[39mwith\u001b[39;00m th\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 338\u001b[0m     actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(observation, deterministic\u001b[39m=\u001b[39;49mdeterministic)\n\u001b[1;32m    339\u001b[0m \u001b[39m# Convert to numpy\u001b[39;00m\n\u001b[1;32m    340\u001b[0m actions \u001b[39m=\u001b[39m actions\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/policies.py:630\u001b[0m, in \u001b[0;36mActorCriticPolicy._predict\u001b[0;34m(self, observation, deterministic)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_predict\u001b[39m(\u001b[39mself\u001b[39m, observation: th\u001b[39m.\u001b[39mTensor, deterministic: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m    623\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    624\u001b[0m \u001b[39m    Get the action according to the policy for a given observation.\u001b[39;00m\n\u001b[1;32m    625\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39m    :return: Taken action according to the policy\u001b[39;00m\n\u001b[1;32m    629\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_distribution(observation)\u001b[39m.\u001b[39mget_actions(deterministic\u001b[39m=\u001b[39mdeterministic)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/policies.py:659\u001b[0m, in \u001b[0;36mActorCriticPolicy.get_distribution\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    657\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_features(obs)\n\u001b[1;32m    658\u001b[0m latent_pi \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp_extractor\u001b[39m.\u001b[39mforward_actor(features)\n\u001b[0;32m--> 659\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_action_dist_from_latent(latent_pi)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/policies.py:607\u001b[0m, in \u001b[0;36mActorCriticPolicy._get_action_dist_from_latent\u001b[0;34m(self, latent_pi)\u001b[0m\n\u001b[1;32m    604\u001b[0m mean_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_net(latent_pi)\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, DiagGaussianDistribution):\n\u001b[0;32m--> 607\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_dist\u001b[39m.\u001b[39;49mproba_distribution(mean_actions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlog_std)\n\u001b[1;32m    608\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist, CategoricalDistribution):\n\u001b[1;32m    609\u001b[0m     \u001b[39m# Here mean_actions are the logits before the softmax\u001b[39;00m\n\u001b[1;32m    610\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dist\u001b[39m.\u001b[39mproba_distribution(action_logits\u001b[39m=\u001b[39mmean_actions)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/stable_baselines3/common/distributions.py:153\u001b[0m, in \u001b[0;36mDiagGaussianDistribution.proba_distribution\u001b[0;34m(self, mean_actions, log_std)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39mCreate the distribution given its parameters (mean, std)\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39m:return:\u001b[39;00m\n\u001b[1;32m    151\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    152\u001b[0m action_std \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mones_like(mean_actions) \u001b[39m*\u001b[39m log_std\u001b[39m.\u001b[39mexp()\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution \u001b[39m=\u001b[39m Normal(mean_actions, action_std)\n\u001b[1;32m    154\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/torch/distributions/normal.py:50\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[0;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 50\u001b[0m \u001b[39msuper\u001b[39;49m(Normal, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/.conda/envs/sustaingym/lib/python3.9/site-packages/torch/distributions/distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     53\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     54\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 55\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m \u001b[39msuper\u001b[39m(Distribution, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 2)) of distribution Normal(loc: torch.Size([1, 2]), scale: torch.Size([1, 2])) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan, nan]], device='cuda:0')"
     ]
    }
   ],
   "source": [
    "# Get rewards for Testing on May 2021\n",
    "\n",
    "model_ppo = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "model_a2c = PPO.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_lst_ppo_21_5 = []\n",
    "rewards_lst_a2c_21_5 = []\n",
    "off_line_rewards_lst_21_5 = []\n",
    "\n",
    "env1 = BatteryStorageInGridEnv(date='2021-05')\n",
    "env2 = BatteryStorageInGridEnv(date='2021-05')\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env1.reset(seed=i)\n",
    "    done = False\n",
    "    rewards = np.zeros(env1.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_ppo.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env1.step(action)\n",
    "        rewards[env1.count - 1] = reward\n",
    "    \n",
    "    obs2 = env2.reset(seed=i)\n",
    "    done2 = False\n",
    "    rewards2 = np.zeros(env2.MAX_STEPS_PER_EPISODE)\n",
    "    while not done2:\n",
    "        action, _states = model_a2c.predict(obs2)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs2, reward, done2, info = env2.step(action)\n",
    "        rewards2[env2.count - 1] = reward\n",
    "    rewards_lst_ppo_21_5.append(np.sum(rewards))\n",
    "    rewards_lst_a2c_21_5.append(np.sum(rewards2))\n",
    "    ob = env1.reset(seed=i)\n",
    "    off_line_rewards_lst_21_5.append(env1._calculate_off_optimal_total_episode_reward())\n",
    "\n",
    "print(\"2021/5 PPO: \", rewards_lst_ppo_21_5)\n",
    "print(\"2021/5 A2C: \", rewards_lst_a2c_21_5)\n",
    "print(\"2021/5 Offline Optimal: \", off_line_rewards_lst_21_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.07it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for PPO trained on May 2019 and tested on May 2019 (in-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05', seed=195)\n",
    "\n",
    "model_ppo = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_ppo_19_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_ppo.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_ppo_19_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.06it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for A2C trained on May 2019 and tested on May 2019 (in-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05', seed=195)\n",
    "\n",
    "model_a2c = A2C.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_a2c_19_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_a2c.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_a2c_19_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:06<00:00,  1.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate off-line optimal values for May 2019\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2019-05', seed=195)\n",
    "\n",
    "offline_optimal_19_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    offline_optimal_19_5.append(env._calculate_off_optimal_total_episode_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for PPO trained on May 2019 and tested on May 2021 (out-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2021-05', seed=215)\n",
    "\n",
    "model_ppo_out = PPO.load(\"ppo_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_ppo_21_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_ppo_out.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_ppo_21_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate evaluation for A2C trained on May 2019 and tested on May 2021 (out-dist)\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2021-05', seed=215)\n",
    "\n",
    "model_a2c_out = A2C.load(\"a2c_single_agent_battery_env\")\n",
    "\n",
    "episodes = 10\n",
    "\n",
    "rewards_a2c_21_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    done = False\n",
    "    rewards = np.zeros(env.MAX_STEPS_PER_EPISODE)\n",
    "    while not done:\n",
    "        action, _states = model_a2c_out.predict(obs)\n",
    "        if action.shape[0] == 1:\n",
    "            action = action.reshape((2,))\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        rewards[env.count - 1] = reward\n",
    "    rewards_a2c_21_5.append(np.sum(rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:07<00:00,  1.39it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from stable_baselines3 import A2C\n",
    "from tqdm import tqdm\n",
    "\n",
    "# generate off-line optimal values for May 2019\n",
    "\n",
    "env = BatteryStorageInGridEnv(date='2021-05', seed=215)\n",
    "\n",
    "offline_optimal_21_5 = []\n",
    "\n",
    "for i in tqdm(range(episodes)):\n",
    "    obs = env.reset(seed=i*10)\n",
    "    offline_optimal_21_5.append(env._calculate_off_optimal_total_episode_reward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/5 PPO:  [2245.0145741723068, 2558.4557707267295, 2536.6470375887716, 2213.9489259221714, 2545.763597560765, 2602.458985199197, 2372.820422435832, 2548.4725186985274, 2261.2602848462757, 2779.4824376099523]\n",
      "2019/5 A2C:  [2245.0145743153016, 2558.4557716921063, 2536.647038219378, 2213.948927097822, 2545.763598501142, 2602.458988265422, 2372.82042346052, 2548.472526844007, 2261.2602856102226, 2779.4824385186284]\n",
      "2019/5 Offline Optimal:  [23051.710297436803, 18248.597237755643, 18994.697055206787, 22868.55334810844, 16514.00674815259, 33131.47019110864, 14612.253560855821, 22389.303426021896, 23079.613407532303, 23031.783863923756]\n",
      "2021/5 PPO:  [2743.0189450456783, 2609.6690354598877, 2641.4290383788198, 2714.706752567427, 2342.052034742505, 2732.207455008233, 2753.6307763219734, 2784.0557684839923, 2722.8755581028713, 2835.9102791492815]\n",
      "2021/5 A2C:  [2743.0189478052616, 2609.6690381383055, 2641.42903982744, 2714.7067542683335, 2342.0520677395284, 2732.207453737356, 2753.6307779419335, 2784.0557688661515, 2722.8755588359236, 2835.9102810094723]\n",
      "2021/5 Offline Optimal:  [20482.670758292246, 19429.36173842518, 16713.67791627479, 22916.353857972834, 18087.993419986706, 20595.192383295172, 17483.9390511944, 21319.578990298218, 22556.99662278885, 21071.943076509524]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"2019/5 PPO: \", rewards_ppo_19_5)\n",
    "print(\"2019/5 A2C: \", rewards_a2c_19_5)\n",
    "print(\"2019/5 Offline Optimal: \", offline_optimal_19_5)\n",
    "print(\"2021/5 PPO: \", rewards_ppo_21_5)\n",
    "print(\"2021/5 A2C: \", rewards_a2c_21_5)\n",
    "print(\"2021/5 Offline Optimal: \", offline_optimal_21_5)\n",
    "\n",
    "models = ['Offline', 'PPO', 'A2C']\n",
    "in_dist = [np.mean(offline_optimal_19_5), np.mean(rewards_ppo_19_5), np.mean(rewards_a2c_19_5)]\n",
    "out_dist = [np.mean(offline_optimal_21_5), np.mean(rewards_ppo_21_5), np.mean(rewards_a2c_21_5)]\n",
    "\n",
    "x_axis = np.arange(len(models))\n",
    "\n",
    "plt.bar(x_axis - 0.2, in_dist, width=0.4, label='In-Dist', color='blue')\n",
    "plt.bar(x_axis + 0.2, out_dist, width=0.4, label='Out-Dist', color='orange')\n",
    "\n",
    "in_dist_err = [np.std(offline_optimal_19_5), np.std(rewards_ppo_19_5), np.std(rewards_a2c_19_5)]\n",
    "out_dist_err = [np.std(offline_optimal_21_5), np.std(rewards_ppo_21_5), np.std(rewards_a2c_21_5)]\n",
    "\n",
    "plt.errorbar(x_axis - 0.2, in_dist, yerr=in_dist_err, fmt='o', color='gray')\n",
    "plt.errorbar(x_axis + 0.2, out_dist, yerr=out_dist_err, fmt='o', color='gray')\n",
    "\n",
    "plt.xticks(x_axis, models)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sustaingym')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c910351b0c3a4aade2cf03b555240fe9951314ae7b50b4f56bc279231ceafe8d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
